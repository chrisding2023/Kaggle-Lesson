type = "response"))
# pick the optimal parameter
accuracy.boost = numeric(100)
for (i in 1:100) {
accuracy.boost[i] = sum(diag(table(OJ.test.indicator$Purchase, boost.predictions[, i]))) / 214
}
min(which(accuracy.boost == max(accuracy.boost)) * 100)
#the minimum number of trees required to reach the maximum accuracy: 1400
gbmpred=predict(boost.OJ, newdata = OJ.test.indicator,n.trees = 1400,type = "response")
pred.test=ifelse (gbmpred > 0.5,1,0)
table(pred.test,OJ.test.indicator$Purchase)
# testing accuracy: (118+61)/214Ôºù83.64%
# xgboost: must use data matrix
# clean data: delete factor varaible and change target variable to numeric
# or you can change factor variable to dummy variable
library(data.table)
library(xgboost)
OJ$StoreID=as.numeric(OJ$StoreID)
OJ$STORE=as.numeric(OJ$STORE)
OJ$Store7=as.numeric(OJ$Store7)
str(OJ)
set.seed(0)
train=sample(1:nrow(OJ), 8*nrow(OJ)/10)
trainOJ=OJ[train,]
testOJ=OJ[-train,]
Purchase.train=OJ[train,]$Purchase
Purchase.test=OJ[-train,]$Purchase
OJ.train.indicator = trainOJ
OJ.test.indicator = testOJ
OJ.train.indicator$Purchase = as.vector(trainOJ$Purchase, mode = "numeric") - 1
OJ.test.indicator$Purchase = as.vector(testOJ$Purchase, mode = "numeric") - 1
train=OJ.train.indicator[, -c(1)]
test=OJ.test.indicator[, -c(1)]
setDT(train)
setDT(test)
new_tr <- model.matrix(~.+0,data = train)
new_ts <- model.matrix(~.+0,data = test)
labels = as.vector(trainOJ$Purchase, mode = "numeric") - 1
ts_label = as.vector(testOJ$Purchase, mode = "numeric") - 1
dtrain <- xgb.DMatrix(data = new_tr,label = labels)
dtest <- xgb.DMatrix(data = new_ts,label=ts_label)
# build the model
params <- list(booster = "gbtree", objective = "binary:logistic", eta=0.001,
max_depth=4,  n_estimators=1400)
#model training
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 1400,
watchlist = list(val=dtest,train=dtrain),
print_every_n = 100, early_stop_round = 100,
maximize = T , eval_metric = "error")
params <- list(booster = "gbtree", objective = "binary:logistic", eta=0.001,
max_depth=4,  n_estimators=400)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 400,
watchlist = list(val=dtest,train=dtrain),
print_every_n = 100, early_stop_round = 100,
maximize = T , eval_metric = "error")
#model prediction
xgbpred <- predict (xgb1,dtest)
xgbpred <- ifelse (xgbpred > 0.5,1,0)
table(ts_label,xgbpred)
# importance
mat <- xgb.importance (feature_names = colnames(new_tr),model = xgb1)
xgb.plot.importance (importance_matrix = mat[1:20])
# The task is to predict the type of a glass on basis of its chemical analysis.
# We start by splitting the data into a train and test set:
library(e1071)
library(rpart)
data(Glass, package="mlbench")
View(Glass)
unique(Glass$Type)
dim(Glass)
str(Glass)
summary(Glass)
## split data into a train and test set
set.seed(0)
index <- 1:nrow(Glass)
testindex <- sample(index, trunc(length(index)/2))
testset <- Glass[testindex,]
trainset <- Glass[-testindex,]
# radial kernel
svm.model <- svm(Type ~ ., data = trainset, cost = 100, gamma = 1)
svm.pred <- predict(svm.model, testset[,-10])
svm.pred
## compute svm confusion matrix
table(pred = svm.pred, true = testset[,10])
# accuracy
(19+30+3+1+1+10)/nrow(testset)
# linear kernel
svm.model1 <- svm(Type ~ ., data = trainset, kernel='linear', cost = 100)
svm.pred1 <- predict(svm.model1, testset[,-10])
## compute svm confusion matrix
table(pred = svm.pred1, true = testset[,10])
# accuracy
(23+27+2+3+3+13)/nrow(testset)
svm.model2 <- svm(Type ~ ., data = trainset, kernel='polynomial',
cost = 100,  degree=2)
svm.pred2 <- predict(svm.model2, testset[,-10])
## compute svm confusion matrix
table(pred = svm.pred2, true = testset[,10])
# accuracy
(19+19+2+3+3+13)/nrow(testset)
# what is the mean and standard deviation of each feature?
iris.meas = iris[, -5]
# data manipulation
#install.packages('nycflight13')
library(nycflights13)
setwd("~/Desktop/kaggle")
Temp=read.csv('Temp.txt', header = T, stringsAsFactors = F, sep=' ')
View(Temp)
dim(Temp)
str(Temp)
summary(Temp)
t.test(Temp$Body.Temp, mu = 98.6, alternative = "two.sided")
t.test(Temp$Body.Temp[Temp$Gender=='Female'],
Temp$Body.Temp[Temp$Gender=='Male'],
alternative = "two.sided")
summary(aov(Body.Temp ~ Gender, data = Temp))
TukeyHSD(aov(Body.Temp ~ Gender, data = Temp))
var.test(Temp$Heart.Rate[Temp$Gender=='Female'],
Temp$Heart.Rate[Temp$Gender=='Male'],
alternative = "two.sided")
var.test(Temp$Heart.Rate[Temp$Gender=='Female'],
Temp$Heart.Rate[Temp$Gender=='Male'],
alternative = "less")
#Question 2
# Load the  HairEyeColordataset located in the  datasetslibrary;
# this is a three dimensional dataset that records the hair color,
# eye color, and gender of 592 different statistics students.
dim(HairEyeColor)
str(HairEyeColor)
mosaicplot(HairEyeColor,shade=TRUE)
#Reduce your dataset to focus on just females with brown and blue eyes
# Conduct a hypothesis test to see if hair and eye color are independent
# of one another for this reduced dataset.
new=HairEyeColor[, c('Brown', 'Blue'), 'Female']
mosaicplot(new,shade=TRUE)
chisq.test(new)
library(PASWR)
data("titanic3")
dim(titanic3)
str(titanic3)
summary(titanic3)
View(titanic3)
titanic3[titanic3=='']=NA
summary(titanic3)
library(mice)
md.pattern(titanic3)
# 7 variables contain at least one missing value
# and they are fare, embarked, age, home.dest, boat, cabin, body
colSums(is.na(titanic3)) * 100 / nrow(titanic3)
colMeans(is.na(titanic3)) * 100
sum(!complete.cases(titanic3))
sum(!complete.cases(titanic3))/ nrow(titanic3)
sum(is.na(titanic3))
mean(is.na(titanic3)) * 100
library(VIM)
aggr(titanic3)
table(titanic3$survived, is.na(titanic3$body))
hist(titanic3$age)
#Impute using mean value imputation for the age variable.
library(Hmisc)
imputed.age = impute(titanic3$age, what=c("mean"))
plot(density(imputed.age), col='blue', main = "Age Distribution")
lines(density(titanic3$age, na.rm = T), col='red')
legend("topright", c("before imputation", "after imputation"), lwd = 1,
lty = 1, col = c("red", "blue"))
#Impute using simple random imputation for the age variable.
set.seed(0)
imputed.age2 = impute(titanic3$age, "random")
plot(density(imputed.age2), col='blue', main = "Age Distribution")
lines(density(titanic3$age, na.rm = T), col='red')
legend("topright", c("before imputation", "after imputation"), lwd = 1,
lty = 1, col = c("red", "blue"))
set.seed(0)
imputed.age2 = impute(titanic3$age, "random")
plot(density(imputed.age2), col='blue', main = "Age Distribution")
lines(density(titanic3$age, na.rm = T), col='red')
legend("topright", c("before imputation", "after imputation"), lwd = 1,
lty = 1, col = c("red", "blue"))
#create a new data frame that includes:
#a. The pclass, survived, sex, age, sibsp, and parch variables from the original titanic3dataset.
#b. The simple random imputation of the fare variable you created above.
set.seed(0)
imputed.fare = impute(titanic3$fare, "random")
new.titanic3=as.data.frame(titanic3$pclass)
colnames(new.titanic3)=c('pclass')
new.titanic3$survived=titanic3$survived
new.titanic3$sex=titanic3$sex
new.titanic3$age=titanic3$age
new.titanic3$sibsp=titanic3$sibsp
new.titanic3$age=titanic3$age
new.titanic3$fare=imputed.fare
summary(new.titanic3)
# Separate this new data frame into two separate data frames as follows
#  (note that there should be no observations that appear in both data frames):
# a. For observations that are totally complete: all variables.
# b. For observations that are missing a value for age: all variables except age.
new.titanic3.com=new.titanic3[complete.cases(new.titanic3),]
dim(new.titanic3.com)
#create a new data frame that includes:
#a. The pclass, survived, sex, age, sibsp, and parch variables from the original titanic3dataset.
#b. The simple random imputation of the fare variable you created above.
set.seed(0)
imputed.fare = impute(titanic3$fare, "random")
new.titanic3=as.data.frame(titanic3$pclass)
colnames(new.titanic3)=c('pclass')
new.titanic3$survived=titanic3$survived
new.titanic3$sex=titanic3$sex
new.titanic3$age=titanic3$age
new.titanic3$sibsp=titanic3$sibsp
new.titanic3$parch=titanic3$parch
new.titanic3$fare=imputed.fare
str(new.titanic3)
summary(new.titanic3)
new.titanic3.incom=new.titanic3[!complete.cases(new.titanic3),-4]
dim(new.titanic3.incom)
# Use 1 Nearest Neighbor to impute using:
#a. Manhattan distance.
#b. Euclidean distance.
#c. Minkowski distance with p = 10 .
library(kknn)
new.titanic3.manhattan = kknn(age ~ ., new.titanic3.com,
new.titanic3.incom, k = 1, distance = 1)
fit1=fitted(new.titanic3.manhattan)
new.titanic3.euclidean = kknn(age ~ ., new.titanic3.com,
new.titanic3.incom, k = 1, distance = 2)
fit2=fitted(new.titanic3.euclidean)
new.titanic3.minkowski=kknn(age ~ ., new.titanic3.com,
new.titanic3.incom, k = 1, distance = 10)
fit3=fitted(new.titanic3.minkowski)
plot(density(new.titanic3.com$age), col='blue', main='Age distribution', ylim=c(0,0.07))
lines(density(fit1), col='red')
lines(density(fit2), col='yellow')
lines(density(fit3), col='green')
legend("topleft", c("com", "manhattan", "euclidean", "minkowski"),lwd = 1,
lty = 1, col = c("blue", "red", "yellow", "green"), cex = .75)
dim(new.titanic3.com)
(1046)^(1/2)
new.titanic3.manhattan1 = kknn(age ~ ., new.titanic3.com,
new.titanic3.incom, k = 32, distance = 1)
fit4=fitted(new.titanic3.manhattan1)
new.titanic3.euclidean1 = kknn(age ~ ., new.titanic3.com,
new.titanic3.incom, k = 32, distance = 2)
fit5=fitted(new.titanic3.euclidean1)
new.titanic3.minkowski1=kknn(age ~ ., new.titanic3.com,
new.titanic3.incom, k = 32, distance = 10)
fit6=fitted(new.titanic3.minkowski1)
plot(density(new.titanic3.com$age), col='blue', main='Age distribution', ylim=c(0,0.1))
lines(density(fit4), col='red')
lines(density(fit5), col='yellow')
lines(density(fit6), col='green')
legend("topleft", c("com", "manhattan", "euclidean", "minkowski"),lwd = 1,
lty = 1, col = c("blue", "red", "yellow", "green"), cex = .75)
#Load the  NYC Restaurants.txtdataset into your workspace.
#This dataset contains survey results from customers of 168 different Italian restaurants
#in the New York City area. The data are in the form of the average of customer views
#on various attributes (food, decor, and service) scored on a scale from 1 to 30,
#along with the average price of dinner.
#There is also a categorical variable for the location of the restaurant.
setwd("~/Desktop/kaggle")
restaurant=read.csv('NYC Restaurants.txt', header = T, stringsAsFactors = F, sep=' ')
head(restaurant)
dim(restaurant)
str(restaurant)
summary(restaurant)
library(dplyr)
#1 Create a scatterplot matrix of all continuous variables colored by Location
restaurant1=dplyr::select(restaurant, Food, Decor, Service )
library(corrplot)
M <- cor(restaurant1)
m
m
M
mcorrplot(M, method="circle")
library(corrplot)
M <- cor(restaurant1)
mcorrplot(M, method="circle")
corrplot(M, method="circle")
unique(restaurant$Location)
#2 Fit a multiple linear regression predicting the price of a meal based on the customer
# views and location of the restaurant.
model1= lm(Price ~ Food+Decor+Service+Location, data = restaurant)
summary(model1)
#3 assumption of MLR
plot(model1)
#4 the influence plot: outliner
library(car)
influencePlot(model1)
restaurant[30,1]
restaurant[56,1]
restaurant[117,1]
restaurant[130,1]
restaurant[168,1]
#5 VIF
vif(model1)
#6 added-variable plot
avPlots(model1)
#7 improvement of model: remove service variable
model3=lm(Price ~ Food+Decor+Location, data = restaurant)
summary(model3)
plot(model3)
influencePlot(model3)
vif(model3)
avPlots(model3)
#8 improvement of model: remove location variable
model4=lm(Price ~ Food+Decor, data = restaurant)
summary(model4)
# the intercept, food and decor coefficients and the overall regression is significant
# (p-values less than 0.05).
#Residual standard error: 5.788 on 165 degrees of freedom
#Adjusted R-squared:  0.6121
plot(model4)
influencePlot(model4)
vif(model4)
avPlots(model4)
#9 model selection
AIC(model1, model3, model4)
BIC(model1, model3, model4)
#We can use stepwise regression to help automate the variable selection process.
#Here we define the minimal model, the full model, and the scope of the models
#through which to search:
model.empty = lm(Price ~ 1, data = restaurant[,-1]) #The model with an intercept ONLY.
model.full = lm(Price ~. , data = restaurant[,-1]) #The model with ALL variables.
scope = list(lower = formula(model.empty), upper = formula(model.full))
library(MASS) #The Modern Applied Statistics library.
#Stepwise regression using AIC as the criteria (the penalty k = 2).
forwardAIC = step(model.empty, scope, direction = "forward", k = 2)
backwardAIC = step(model.full, scope, direction = "backward", k = 2)
bothAIC.empty = step(model.empty, scope, direction = "both", k = 2)
bothAIC.full = step(model.full, scope, direction = "both", k = 2)
#Stepwise regression using BIC as the criteria (the penalty k = log(n)).
forwardBIC = step(model.empty, scope, direction = "forward", k = log(168))
backwardBIC = step(model.full, scope, direction = "backward", k = log(168))
bothBIC.empty = step(model.empty, scope, direction = "both", k = log(168))
bothBIC.full = step(model.full, scope, direction = "both", k = log(168))
setwd("~/Desktop/kaggle")
Prostate=read.csv('Prostate.txt', header = T, stringsAsFactors = F, sep='')
Prostate=read.csv('Prostate.txt', header = T, stringsAsFactors = F, sep='')
View(Prostate)
dim(Prostate)
str(Prostate)
summary(Prostate)
#1 Creating the data matrices for the glmnet() function.
library(ISLR)
library(glmnet)
x=model.matrix(lpsa~., Prostate)[, -1]
x
y=Prostate$lpsa
# separate to training/test by ratio 4:1
set.seed(0)
train = sample(1:nrow(x), 8*nrow(x)/10)
y.test = y[-train]
y.train=y[train]
length(train)/nrow(x) #0.8
length(y.test)/nrow(x) #0.2
#2 Fit a slew of ridge regression models  on your training data by checking
# across a wide range of lambda values
grid = 10^seq(5, -2, length = 100)
grid
ridge.models = glmnet(x[train, ], y[train], alpha = 0, lambda = grid)
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
#3 Perform 10-fold cross-validation on your training data
# and save the output as an object
set.seed(0)
set.seed(0)
cv.ridge.out = cv.glmnet(x[train, ], y[train],
lambda = grid, alpha = 0, nfolds = 10)
plot(cv.ridge.out, main = "Ridge Regression\n")
#The error seems to be reduced with a log lambda value of around -1.84, This is the value of lambda
#we should move forward with in our future analyses.
bestlambda.ridge = cv.ridge.out$lambda.min
bestlambda.ridge
# best lambda: 0.1592283
log(bestlambda.ridge)
ridge.out = glmnet(x[train,], y[train], alpha = 0, lambda = bestlambda.ridge)
predict(ridge.out, type = "coefficients", s = bestlambda.ridge)
#3 test MSE
ridge.bestlambdatrain = predict(ridge.out, s = bestlambda.ridge, newx = x[-train, ])
mean((ridge.bestlambdatrain - y.test)^2)
# test MSE:0.598976
ridge.bestlambdatrain = predict(ridge.out, s = bestlambda.ridge, newx = x[train, ])
mean((ridge.bestlambdatrain - y.train)^2)
#4 lasso regression
lasso.models = glmnet(x[train, ], y[train], alpha = 1, lambda = grid)
plot(lasso.models, xvar = "lambda", label = TRUE, main = "Lasso Regression")
#The coefficients all seem to shrink towards 0 as lambda gets quite large. All
#coefficients seem to go to 0 once the log lambda value gets to about 0. We
#note that in the lasso regression scenario, coefficients are necessarily set
#to exactly 0.
set.seed(0)
cv.lasso.out = cv.glmnet(x[train, ], y[train], alpha = 1, nfolds = 10, lambda = grid)
plot(cv.lasso.out, main = "Lasso Regression\n")
bestlambda.lasso = cv.lasso.out$lambda.min
bestlambda.lasso
# 0.05994843
log(bestlambda.lasso)
lasso.out = glmnet(x[train,], y[train], alpha = 1, lambda = bestlambda.lasso)
predict(lasso.out, type = "coefficients", s = bestlambda.lasso)
lasso.bestlambdatrain = predict(lasso.out, s = bestlambda.lasso, newx = x[-train, ])
mean((lasso.bestlambdatrain - y.test)^2)
# MSE test:0.6367398
lasso.bestlambdatrain = predict(lasso.out, s = bestlambda.lasso, newx = x[train, ])
# MSE test:0.6367398
lasso.bestlambdatrain = predict(lasso.out, s = bestlambda.lasso, newx = x[train, ])
mean((lasso.bestlambdatrain - y.train)^2)
lasso.bestlambdatrain = predict(lasso.out, s = bestlambda.lasso, newx = x[-train, ])
mean((lasso.bestlambdatrain - y.test)^2)
library(HSAUR)
heptathlon
names(heptathlon)
plot(heptathlon)
# It will help to have all event scores going in the ‚Äúsame direction‚Äù
#(i.e., a higher event score implies a better performance, and a lower
#event score implies a worse performance). To do so, transform the hurdle
# and running variables by subtracting the original scores
#for each heptathlete from the maximum score of each of those variables
heptathlon.new = heptathlon
heptathlon.new$hurdles = max(heptathlon$hurdles) - heptathlon$hurdles
heptathlon.new$run200m = max(heptathlon$run200m) - heptathlon$run200m
heptathlon.new$run800m = max(heptathlon$run800m) - heptathlon$run800m
heptathlon.new
plot(heptathlon.new)
cor(heptathlon.new)
round(cor(heptathlon.new[,-8]), 2)
# Extract the appropriate number of principal components from your dataset
# that does not include the  score  variable and save this object.
library(psych)
fa.parallel(heptathlon.new[, -8], fa = "pc", n.iter = 100)
# show the eigen values for a principal components using fa = "pc"
abline(h = 1)
pc_heptathlon = principal(heptathlon.new[, -8], nfactors = 2, rotate = "none")
pc_heptathlon
factor.plot(pc_heptathlon, labels = colnames(heptathlon.new))
plot(pc_heptathlon$scores)
pc_heptathlon$scores
cor(pc_heptathlon$scores)
setwd("~/Desktop/ËØæ‰ª∂")
protein = read.table("[08] Protein.txt", sep = "\t", header = TRUE)
dim(protein)
str(protein)
summary(protein)
wssplot = function(data, nc = 15, seed = 0) {
wss = (nrow(data) - 1) * sum(apply(data, 2, var))
for (i in 2:nc) {
set.seed(seed)
wss[i] = sum(kmeans(data, centers = i, iter.max = 100, nstart = 100)$withinss)
}
plot(1:nc, wss, type = "b",
xlab = "Number of Clusters",
ylab = "Within-Cluster Variance",
main = "Scree Plot for the K-Means Procedure")
}
wssplot(protein)
wssplot(protein[,-1])
#Running the K-means procedure 5 different times, but with only one convergence of the
# algorithm each time.
set.seed(0)
km.protein1 = kmeans(protein.scaled, centers = 3)
km.protein2 = kmeans(protein.scaled, centers = 3)
km.protein3 = kmeans(protein.scaled, centers = 3)
km.protein4 = kmeans(protein.scaled, centers = 3)
km.protein5 = kmeans(protein.scaled, centers = 3)
#Running the K-means procedure 100 times
set.seed(0)
km.proteinsim = kmeans(protein.scaled, centers = 3, nstart = 100)
km.protein1 = kmeans(protein[,-1], centers = 3)
km.protein2 = kmeans(protein[,-1], centers = 3)
km.protein3 = kmeans(protein[,-1], centers = 3)
km.protein4 = kmeans(protein[,-1], centers = 3)
km.protein5 = kmeans(protein[,-1], centers = 3)
#Running the K-means procedure 100 times
set.seed(0)
km.proteinsim = kmeans(protein[,-1], centers = 3, nstart = 100)
par(mfrow = c(2, 3))
par(mfrow = c(2, 3))
plot(protein[,-1]$Cereals, protein[,-1]$RedMeat, col = km.protein1$cluster,
xlab = "Cereal Consumption", ylab = "Red Meat Consumption",
main = paste("Single K-Means Attempt #1\n WCV: ",
round(km.protein1$tot.withinss, 4)))
plot(protein[,-1]$Cereals, protein[,-1]$RedMeat, col = km.protein2$cluster,
xlab = "Cereal Consumption", ylab = "Red Meat Consumption",
main = paste("Single K-Means Attempt #2\n WCV: ",
round(km.protein2$tot.withinss, 4)))
plot(protein[,-1]$Cereals,protein[,-1]$RedMeat, col = km.protein3$cluster,
xlab = "Cereal Consumption", ylab = "Red Meat Consumption",
main = paste("Single K-Means Attempt #3\n WCV: ",
round(km.protein3$tot.withinss, 4)))
plot(protein[,-1]$Cereals, protein[,-1]$RedMeat, col = km.protein4$cluster,
xlab = "Cereal Consumption", ylab = "Red Meat Consumption",
main = paste("Single K-Means Attempt #4\n WCV: ",
round(km.protein4$tot.withinss, 4)))
plot(protein[,-1]$Cereals, protein[,-1]$RedMeat, col = km.protein5$cluster,
xlab = "Cereal Consumption", ylab = "Red Meat Consumption",
main = paste("Single K-Means Attempt #5\n WCV: ",
round(km.protein5$tot.withinss, 4)))
plot(protein[,-1]$Cereals, protein[,-1]$RedMeat, col = km.proteinsim$cluster,
xlab = "Cereal Consumption", ylab = "Red Meat Consumption",
main = paste("Best K-Means Attempt out of 100\n WCV: ",
round(km.proteinsim$tot.withinss, 4)))
par(mfrow = c(1, 1))
plot(protein[,-1]$Cereals, protein[,-1]$RedMeat, type = "n",
xlab = "Cereal Consumption", ylab = "Red Meat Consumption",
main = paste("Best K-Means Attempt out of 100\n WCV: ",
round(km.proteinsim$tot.withinss, 4)))
points(km.proteinsim$centers[, 6], km.proteinsim$centers[, 1], pch = 16, col = "blue")
abline(h = 0, lty = 2)
abline(v = 0, lty = 2)
text(protein[,-1]$Cereals, protein[,-1]$RedMeat,
labels =protein[,1],
col = km.proteinsim$cluster)
setwd("~/Desktop/harmony plus kaggle 623")
# set working directory
getwd() # if not set, the default is root directory
